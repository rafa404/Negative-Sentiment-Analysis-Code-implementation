{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30d96c8",
   "metadata": {},
   "source": [
    "deberta + lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_absolute_error, matthews_corrcoef, r2_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAINED = 'microsoft/deberta-base'\n",
    "MAX_LEN    = 64\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS     = 3\n",
    "LR         = 2e-5\n",
    "DATA_PATH  = r'D:\\Thesis\\16-7-25\\thesis 2.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df['Text'].astype(str).tolist()\n",
    "y = df['Label'].astype(int).tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        enc = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "train_ds = SentimentDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "test_ds  = SentimentDataset(X_test,  y_test,  tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "class DebertaLSTMClassifier(nn.Module):\n",
    "    def __init__(self, deberta_model, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.deberta = deberta_model\n",
    "        H = deberta_model.config.hidden_size\n",
    "        self.bilstm = nn.LSTM(H, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.drop   = nn.Dropout(0.3)\n",
    "        self.fc     = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            out = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = out.last_hidden_state\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.drop(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "deberta_model = AutoModel.from_pretrained(PRETRAINED)\n",
    "deberta_model.gradient_checkpointing_enable()\n",
    "model = DebertaLSTMClassifier(deberta_model.to(DEVICE), hidden_dim=64, num_classes=len(set(y))).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        lbls = batch['label'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss = criterion(logits, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == lbls).sum().item()\n",
    "        total += lbls.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    train_accuracies.append(acc)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_loader):.4f} - Train Acc: {acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred, y_prob = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        lbls = batch['label'].to(DEVICE)\n",
    "\n",
    "        logits = model(ids, mask)\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        preds  = torch.argmax(probs, dim=1)\n",
    "\n",
    "        y_true.extend(lbls.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_prob.extend(probs.cpu().numpy())\n",
    "\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='macro')\n",
    "rec  = recall_score(y_true, y_pred, average='macro')\n",
    "f1   = f1_score(y_true, y_pred, average='macro')\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mcc  = matthews_corrcoef(y_true, y_pred)\n",
    "r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\nðŸ“Š Final Test Metrics:\")\n",
    "print(f\" Accuracy       : {acc:.4f}\")\n",
    "print(f\" Precision      : {prec:.4f}\")\n",
    "print(f\" Recall         : {rec:.4f}\")\n",
    "print(f\" F1 Score       : {f1:.4f}\")\n",
    "print(f\" Mean Abs Error : {mae:.4f}\")\n",
    "print(f\" MCC            : {mcc:.4f}\")\n",
    "print(f\" R\\u00b2 Score       : {r2:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, EPOCHS + 1), train_accuracies, marker='o', color='green')\n",
    "plt.title('Training Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "if len(set(y_true)) == 2:\n",
    "    auc = roc_auc_score(y_true, [p[1] for p in y_prob])\n",
    "    print(f\"ROC AUC Score: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8a7a7",
   "metadata": {},
   "source": [
    "ALBERT + BiGRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a65c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAINED = 'albert-base-v2'\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "DATA_PATH = r'D:\\Thesis\\16-7-25\\thesis 2.csv'  # Your CSV file path\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df['Text'].astype(str).tolist()\n",
    "labels = df['Label'].astype(int).tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare tokenizer and data loaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "train_ds = SentimentDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "test_ds = SentimentDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model: ALBERT + BiGRU\n",
    "class AlbertGRUClassifier(nn.Module):\n",
    "    def __init__(self, albert_model, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.albert = albert_model\n",
    "        hidden_size = albert_model.config.hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "        gru_out, _ = self.gru(sequence_output)       # [batch, seq_len, hidden_dim*2]\n",
    "        avg_pool = torch.mean(gru_out, dim=1)        # mean over seq_len\n",
    "        x = self.dropout(avg_pool)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Instantiate model\n",
    "albert_model = AutoModel.from_pretrained(PRETRAINED)\n",
    "model = AlbertGRUClassifier(albert_model, hidden_dim=128, num_layers=1, num_classes=len(set(labels))).to(DEVICE)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track training accuracy for plotting\n",
    "train_acc_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    epoch_acc = total_correct / total_samples\n",
    "    train_acc_list.append(epoch_acc)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} â€” Loss: {total_loss/len(train_loader):.4f} â€” Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='macro')\n",
    "rec = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "# Plot training accuracy curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), train_acc_list, marker='o', label='Train Accuracy')\n",
    "plt.title('Training Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(range(1, EPOCHS + 1))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1069eb",
   "metadata": {},
   "source": [
    "ELECTRA + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAINED = \"google/electra-base-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "DATA_PATH = r\"D:\\Thesis\\16-7-25\\thesis 2.csv\"\n",
    "\n",
    "# Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Model\n",
    "class ElectraBiLSTM(nn.Module):\n",
    "    def __init__(self, electra_model, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.electra = electra_model\n",
    "        hidden_size = electra_model.config.hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = out.last_hidden_state\n",
    "        lstm_out, _ = self.lstm(sequence_output)\n",
    "        avg_pool = torch.mean(lstm_out, dim=1)\n",
    "        x = self.dropout(avg_pool)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df[\"Text\"].astype(str).tolist()\n",
    "labels = df[\"Label\"].astype(int).tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "train_ds = SentimentDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "test_ds = SentimentDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "electra_model = AutoModel.from_pretrained(PRETRAINED)\n",
    "model = ElectraBiLSTM(electra_model, hidden_dim=128, num_layers=1, num_classes=len(set(labels))).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_acc_list = []\n",
    "\n",
    "# Train\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_acc = correct / total\n",
    "    train_acc_list.append(epoch_acc)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} â€” Loss: {total_loss/len(train_loader):.4f} â€” Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average=\"macro\")\n",
    "rec = recall_score(y_true, y_pred, average=\"macro\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "# Plot training accuracy curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, EPOCHS+1), train_acc_list, marker='o', label='Train Accuracy')\n",
    "plt.title('Training Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(range(1, EPOCHS+1))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd3db3",
   "metadata": {},
   "source": [
    "BERT + SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PRETRAINED = \"bert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "DATA_PATH = r\"D:\\Thesis\\16-7-25\\thesis 2.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df[\"Text\"].astype(str).tolist()\n",
    "labels = df[\"Label\"].astype(int).tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer + model for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "model = AutoModel.from_pretrained(PRETRAINED).to(device)\n",
    "model.eval()\n",
    "\n",
    "def embed_texts(texts, tokenizer, model, max_len):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_len, truncation=True, padding=\"max_length\")\n",
    "        inputs = {k: v.to(device) for k,v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_emb = outputs.last_hidden_state[:,0,:].cpu().numpy()\n",
    "            embeddings.append(cls_emb.squeeze())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "print(\"Embedding training data...\")\n",
    "X_train_emb = embed_texts(X_train, tokenizer, model, MAX_LEN)\n",
    "print(\"Embedding test data...\")\n",
    "X_test_emb = embed_texts(X_test, tokenizer, model, MAX_LEN)\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train_emb, y_train)\n",
    "\n",
    "# Train accuracy\n",
    "y_train_pred = svm.predict(X_train_emb)\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svm.predict(X_test_emb)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"Test Accuracy:\", acc)\n",
    "print(\"Test Precision:\", prec)\n",
    "print(\"Test Recall:\", rec)\n",
    "print(\"Test F1:\", f1)\n",
    "\n",
    "# Confusion matrix plot\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c5f8e",
   "metadata": {},
   "source": [
    "ERNIE + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, mean_absolute_error, r2_score,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAINED = \"nghuyong/ernie-1.0\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "DATA_PATH = \"thesis.csv\"\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "class ERNIE_CNN(nn.Module):\n",
    "    def __init__(self, ernie_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.ernie = ernie_model\n",
    "        hidden_size = ernie_model.config.hidden_size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=hidden_size, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.ernie(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df[\"Text\"].astype(str).tolist()\n",
    "labels = df[\"Label\"].astype(int).tolist()\n",
    "num_classes = len(set(labels))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "train_ds = SentimentDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "test_ds = SentimentDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "ernie_model = AutoModel.from_pretrained(PRETRAINED)\n",
    "model = ERNIE_CNN(ernie_model, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_true_all = []\n",
    "train_pred_all = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        train_true_all.extend(labels.cpu().numpy())\n",
    "        train_pred_all.extend(preds.cpu().numpy())\n",
    "\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} â€” Loss: {total_loss/len(train_loader):.4f} â€” Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Calculate train metrics\n",
    "train_prec = precision_score(train_true_all, train_pred_all, average=\"macro\")\n",
    "train_rec = recall_score(train_true_all, train_pred_all, average=\"macro\")\n",
    "train_f1 = f1_score(train_true_all, train_pred_all, average=\"macro\")\n",
    "train_mcc = matthews_corrcoef(train_true_all, train_pred_all)\n",
    "train_mae = mean_absolute_error(train_true_all, train_pred_all)\n",
    "train_r2 = r2_score(train_true_all, train_pred_all)\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"Precision: {train_prec:.4f}\")\n",
    "print(f\"Recall:    {train_rec:.4f}\")\n",
    "print(f\"F1 Score:  {train_f1:.4f}\")\n",
    "print(f\"MCC:       {train_mcc:.4f}\")\n",
    "print(f\"MAE:       {train_mae:.4f}\")\n",
    "print(f\"R2 Score:  {train_r2:.4f}\")\n",
    "\n",
    "# Evaluation on Test\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "y_prob = []  # For AUC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_prob.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "test_prec = precision_score(y_true, y_pred, average=\"macro\")\n",
    "test_rec = recall_score(y_true, y_pred, average=\"macro\")\n",
    "test_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "test_mcc = matthews_corrcoef(y_true, y_pred)\n",
    "test_mae = mean_absolute_error(y_true, y_pred)\n",
    "test_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_prec:.4f}\")\n",
    "print(f\"Recall:    {test_rec:.4f}\")\n",
    "print(f\"F1 Score:  {test_f1:.4f}\")\n",
    "print(f\"MCC:       {test_mcc:.4f}\")\n",
    "print(f\"MAE:       {test_mae:.4f}\")\n",
    "print(f\"R2 Score:  {test_r2:.4f}\")\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "classes = list(set(labels))\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC AUC Curve for Multiclass\n",
    "y_true_bin = label_binarize(y_true, classes=classes)\n",
    "y_prob = np.array(y_prob)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_auc_score([y_true_bin[:, i]], [y_prob[:, i]])\n",
    "    roc_auc[i] = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "\n",
    "plt.figure()\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label=f'Class {classes[i]} ROC curve (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
