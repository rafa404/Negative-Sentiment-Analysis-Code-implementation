{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc1eb75",
   "metadata": {},
   "source": [
    "CNN + Self-Attention Text Classifier (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, mean_absolute_error, matthews_corrcoef, r2_score\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_PATH = r\"D:\\Thesis\\16-7-25\\thesis 2.csv\"\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df['Text'].astype(str).tolist()\n",
    "labels = df['Label'].astype(int).tolist()\n",
    "num_classes = len(set(labels))\n",
    "\n",
    "tokenizer = lambda x: x.lower().split()\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for text in texts:\n",
    "    for word in tokenizer(text):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "def encode(text, max_len):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    ids = ids[:max_len] + [vocab['<PAD>']] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "X = [encode(t, MAX_LEN) for t in texts]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = embed_dim ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(weights, V)\n",
    "\n",
    "class CNNAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=5, padding=2)\n",
    "        self.attn = SelfAttention(128)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                    # (B, T, E)\n",
    "        x = x.permute(0, 2, 1)                   # (B, E, T)\n",
    "        x = torch.relu(self.conv(x))            # (B, C, T)\n",
    "        x = x.permute(0, 2, 1)                   # (B, T, C)\n",
    "        x = self.attn(x)                         # (B, T, C)\n",
    "        x = x.permute(0, 2, 1)                   # (B, C, T)\n",
    "        x = self.pool(x).squeeze(-1)             # (B, C)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = CNNAttentionClassifier(len(vocab), embed_dim=128, num_classes=num_classes).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_acc_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_correct, total = 0, 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    acc = total_correct / total\n",
    "    train_acc_list.append(acc)\n",
    "    print(f\"Epoch {epoch+1} | Train Accuracy: {acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='macro')\n",
    "rec = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nFinal Test Metrics:\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"MAE      : {mae:.4f}\")\n",
    "print(f\"MCC      : {mcc:.4f}\")\n",
    "print(f\"R² Score : {r2:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "minutes = total_time // 60\n",
    "seconds = total_time % 60\n",
    "print(f\"\\nTotal time taken: {int(minutes)} minutes and {int(seconds)} seconds\")\n",
    "\n",
    "\n",
    "plt.plot(range(1, EPOCHS + 1), train_acc_list, marker='o')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec21359",
   "metadata": {},
   "source": [
    "LSTM + ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c346c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, mean_absolute_error, matthews_corrcoef, r2_score\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_PATH = r\"D:\\Thesis\\16-7-25\\thesis 2.csv\"\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df['Text'].astype(str).tolist()\n",
    "labels = df['Label'].astype(int).tolist()\n",
    "num_classes = len(set(labels))\n",
    "\n",
    "# Simple tokenizer + build vocab\n",
    "tokenizer = lambda x: x.lower().split()\n",
    "vocab = {'<PAD>':0, '<UNK>':1}\n",
    "for text in texts:\n",
    "    for word in tokenizer(text):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "def encode(text, max_len):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [vocab.get(t, vocab['<UNK>']) for t in tokens]\n",
    "    ids = ids[:max_len] + [vocab['<PAD>']]*(max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "X = [encode(t, MAX_LEN) for t in texts]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Self-attention layer\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)  # (B, T, H)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (B, T, T)\n",
    "        weights = torch.softmax(scores, dim=-1)                    # (B, T, T)\n",
    "        out = torch.matmul(weights, V)                             # (B, T, H)\n",
    "        return out\n",
    "\n",
    "# Model: Embedding + BiLSTM + SelfAttention + Classifier\n",
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = SelfAttention(hidden_dim*2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)             # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(emb)       # (B, T, 2*H)\n",
    "        attn_out = self.attention(lstm_out) # (B, T, 2*H)\n",
    "        # Pooling (mean over time)\n",
    "        pooled = attn_out.mean(dim=1)       # (B, 2*H)\n",
    "        dropped = self.dropout(pooled)\n",
    "        logits = self.fc(dropped)           # (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "model = LSTMAttentionClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=64,\n",
    "    num_classes=num_classes\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_acc_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_correct, total = 0, 0\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "    acc = total_correct / total\n",
    "    train_acc_list.append(acc)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Train Accuracy: {acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='macro')\n",
    "rec = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nFinal Test Metrics:\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"MAE      : {mae:.4f}\")\n",
    "print(f\"MCC      : {mcc:.4f}\")\n",
    "print(f\"R² Score : {r2:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "minutes = total_time // 60\n",
    "seconds = total_time % 60\n",
    "print(f\"\\nTotal time taken: {int(minutes)} minutes and {int(seconds)} seconds\")\n",
    "\n",
    "plt.plot(range(1, EPOCHS+1), train_acc_list, marker='o')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
