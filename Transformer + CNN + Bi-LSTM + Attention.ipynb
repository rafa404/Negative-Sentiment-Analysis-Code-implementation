{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xBa7Bp0KCXu"
      },
      "outputs": [],
      "source": [
        "Transformer + CNN + Bi-LSTM + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Install dependencies\n",
        "!pip install -q transformers tensorflow xgboost scikit-learn\n",
        "\n",
        "# üìö Imports\n",
        "import numpy as np, pandas as pd, gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Input, Conv1D, GlobalMaxPooling1D,\n",
        "                                     Bidirectional, LSTM, Dense, Dropout,\n",
        "                                     Attention, Layer, Concatenate)\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import xgboost as xgb\n",
        "\n",
        "# üß≠ 1. Load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/thesis/thesis 2.csv')\n",
        "df = df[['Text','Label']].dropna()\n",
        "X, y = df['Text'].tolist(), df['Label'].values\n",
        "Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# üß† 2. Transformer embeddings\n",
        "MODEL = 'roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "bert = TFAutoModel.from_pretrained(MODEL)\n",
        "\n",
        "def get_emb(texts, max_len=128, bs=16):\n",
        "    out = []\n",
        "    for i in range(0, len(texts), bs):\n",
        "        b = texts[i:i+bs]\n",
        "        tk = tokenizer(b, padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
        "        h = bert(tk['input_ids'], attention_mask=tk['attention_mask']).last_hidden_state\n",
        "        out.append(h.numpy()); del tk, h; gc.collect()\n",
        "    return np.vstack(out)\n",
        "\n",
        "Xt_emb, Xv_emb = get_emb(Xt), get_emb(Xv)\n",
        "\n",
        "# üéØ 3. Attention layer\n",
        "class SelfAtt(Layer):\n",
        "    def __init__(self, units): super().__init__(); self.Wq=Dense(units); self.Wk=Dense(units); self.Wv=Dense(units); self.att=Attention()\n",
        "    def call(self, x):\n",
        "        return self.att([self.Wq(x), self.Wk(x), self.Wv(x)])\n",
        "\n",
        "# üß± 4. Dual-branch model\n",
        "def get_dual_out(inp):\n",
        "    # CNN branch\n",
        "    x = Conv1D(128, 3, activation='relu', padding='same')(inp)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    cnn = Dropout(0.3)(x)\n",
        "    # BiLSTM + Attention branch\n",
        "    y = Bidirectional(LSTM(64, return_sequences=True))(inp)\n",
        "    y = Dropout(0.3)(y)\n",
        "    a = SelfAtt(64)(y)\n",
        "    a = GlobalMaxPooling1D()(a)\n",
        "    lstm = Dense(64, activation='relu')(a)\n",
        "    return cnn, lstm\n",
        "\n",
        "inp = Input(shape=(128, bert.config.hidden_size))\n",
        "c_out, l_out = get_dual_out(inp)\n",
        "merged = Concatenate()([c_out, l_out])\n",
        "feat_model = Model(inp, merged)\n",
        "\n",
        "# üîç 5. Extract features\n",
        "Xtf = feat_model.predict(Xt_emb, batch_size=16, verbose=1)\n",
        "Xvf = feat_model.predict(Xv_emb, batch_size=16, verbose=1)\n",
        "\n",
        "# üõ† 6. Train XGBoost and evaluate\n",
        "clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "clf.fit(Xtf, yt)\n",
        "yv_pred = clf.predict(Xvf)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(yv, yv_pred))\n",
        "print(\"Precision:\", precision_score(yv, yv_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(yv, yv_pred, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(yv, yv_pred, average='weighted'))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(yv, yv_pred))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
